{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing TIRG\n",
    "\n",
    "> Refer to《Composing Text and Image for Image Retrieval - An Empirical Odyssey》\n",
    "\n",
    "In this work, we consider the case where queries are formulated as an input image plus a text string that describes some desired modification to the image.\n",
    "\n",
    "The steps in the process are as follows:\n",
    "\n",
    "1. Data Loading ;  \n",
    "2. Model Building ;  \n",
    "3. Model Training ;  \n",
    "4. Model Evaluating ;  \n",
    "5. Image Retrieving  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the configuration information of environment  \n",
    "Our environments are as follows:\n",
    "\n",
    "* Python version: 3.7.10  \n",
    "* PyTorch version: 1.8.1  \n",
    "* CUDA used to build PyTorch: 10.2  \n",
    "* pytorch-lightning: 1.2.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "PyTorch version: 1.8.1\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 10.2\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 10 家庭中文版\n",
      "GCC version: Could not collect\n",
      "Clang version: 10.0.0 \n",
      "CMake version: version 3.19.6\n",
      "\n",
      "Python version: 3.7 (64-bit runtime)\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "GPU models and configuration: GPU 0: GeForce RTX 2070 Super\n",
      "Nvidia driver version: 461.72\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.20.2\n",
      "[pip3] pytorch-lightning==1.2.5\n",
      "[pip3] torch==1.8.1\n",
      "[pip3] torchmetrics==0.2.0\n",
      "[pip3] torchvision==0.9.1\n",
      "[conda] blas                      2.108                       mkl    conda-forge\n",
      "[conda] blas-devel                3.9.0                     8_mkl    conda-forge\n",
      "[conda] cudatoolkit               10.2.89              hb195166_8    conda-forge\n",
      "[conda] libblas                   3.9.0                     8_mkl    conda-forge\n",
      "[conda] libcblas                  3.9.0                     8_mkl    conda-forge\n",
      "[conda] liblapack                 3.9.0                     8_mkl    conda-forge\n",
      "[conda] liblapacke                3.9.0                     8_mkl    conda-forge\n",
      "[conda] mkl                       2020.4             hb70f87d_311    conda-forge\n",
      "[conda] mkl-devel                 2020.4             h57928b3_312    conda-forge\n",
      "[conda] mkl-include               2020.4             hb70f87d_311    conda-forge\n",
      "[conda] numpy                     1.20.2           py37hcbcd69c_0    conda-forge\n",
      "[conda] pytorch                   1.8.1           py3.7_cuda10.2_cudnn7_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\n",
      "[conda] pytorch-lightning         1.2.5                    pypi_0    pypi\n",
      "[conda] torchmetrics              0.2.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.9.1                py37_cu102    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import collect_env\n",
    "collect_env.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET\n",
    "### Fashion IQ\n",
    "The dataset contains diverse fashion images (**dresses, shirts, and tops&tees**), side information in form of textual descriptions and product meta-data, attribute labels, and most importantly, large-scale annotations of high quality relative captions collected from human annotators.  \n",
    "\n",
    "Refer to 《The Fashion IQ Dataset: Retrieving Images by Combining Side Information and Relative Natural Language Feedback》and [Fashion IQ 数据集介绍及处理](https://invisprints.vercel.app/fashion-iq)\n",
    "\n",
    "### Import relvant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">torch.utils.data.Dataset  \n",
    "\n",
    "An abstract class representing a Dataset.  \n",
    "\n",
    "All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite **\\_\\_getitem\\_\\_()**, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite **\\_\\_len\\_\\_()**, which is expected to return the size of the dataset by many Sampler implementations and the default options of DataLoader.  \n",
    "\n",
    "### Create a class for FashionIQ Dataset, read the caption content in \\_\\_init\\_\\_()  and load the image in \\_\\_getitem\\_\\_() . This is done to save memory usage, loading images when needed instead of loading them at first. The data will be organized into dict as {'c_img': c_img, 't_img': t_img, 'encoded_caption': encoded_caption}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionIQDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, split, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.transform = kwargs.get(\"transform\", None)\n",
    "        self.test_targets = kwargs.get(\"test_targets\", False)\n",
    "        self.split = split\n",
    "        self.data_name = kwargs.get('data_name', ['dress', 'shirt', 'toptee'])    #self.data_name=['dress', 'shirt', 'toptee']\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')   #Load pretrained model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        \n",
    "        #using split as a flag to train or test\n",
    "        if split is 'train': # train dataset data = [dress + shirt + toptee]\n",
    "            assert isinstance(self.data_name, list), 'data_name must be list, get {}'.format(type(self.data_name))\n",
    "            for data_name in self.data_name:\n",
    "                cap_file = '{}/captions/cap.{}.{}.json'.format(data_path, data_name, split)\n",
    "                with open(cap_file, 'r') as f: data = json.load(f)\n",
    "                for d in tqdm(data, desc='loading {} {} data'.format(data_name, split)):\n",
    "                    c_name = d['candidate']\n",
    "                    t_name = d['target']\n",
    "                    text = [x.strip() for x in d['captions']]\n",
    "                    text = ' [SEP] '.join(text)\n",
    "                    #e.g. is solid black with no sleeves [SEP] is black with straps\n",
    "                    self.data.append({'c_path': os.path.join(data_path, 'resized_images', data_name, c_name+'.jpg'),\n",
    "                                      't_path': os.path.join(data_path, 'resized_images', data_name, t_name+'.jpg'),\n",
    "                                      'encoded_caption': tokenizer(text)}) \n",
    "                    # Don't use pt here,for pt is 2-d array,but tokenizer.pad can only joint 1-d tensor\n",
    "                              \n",
    "        else: # test part, one of 'dress', 'shirt' and 'toptee'\n",
    "            assert isinstance(self.data_name, str), 'data_name must be string, get {}'.format(type(self.data_name))\n",
    "            split_file = '{}/image_splits/split.{}.{}.json'.format(data_path, self.data_name, split)\n",
    "            with open(split_file, 'r') as f: self.names = json.load(f)\n",
    "                              \n",
    "            if self.test_targets: # test_targets\n",
    "                for i, name in enumerate(tqdm(self.names, desc='loading {} {} pool data'.format(self.data_name, split))):\n",
    "                    self.data.append({'t_id': i,\n",
    "                                      't_path': os.path.join(data_path, 'resized_images', self.data_name, name+'.jpg')})\n",
    "            else: # test_quaries \n",
    "                cap_file = '{}/captions/cap.{}.{}.json'.format(data_path, self.data_name, split)\n",
    "                with open(cap_file, 'r') as f: data = json.load(f)\n",
    "                for d in tqdm(data, desc='generating {} {} data'.format(self.data_name, split)):\n",
    "                    c_name = d['candidate']\n",
    "                    t_name = d['target']\n",
    "                    text = [x.strip() for x in d['captions']]\n",
    "                    text = ' [SEP] '.join(text)\n",
    "                    self.data.append({'c_path': os.path.join(data_path, 'resized_images', self.data_name, c_name+'.jpg'),\n",
    "                                     'c_id': self.names.index(c_name),\n",
    "                                     't_path': os.path.join(data_path, 'resized_images', self.data_name, t_name+'.jpg'),\n",
    "                                     't_id': self.names.index(t_name),\n",
    "                                     'encoded_caption': tokenizer(text)})\n",
    "                \n",
    "        print('Statistics: collected in {} {}, {} datas found'.format(self.data_name, self.split, len(self.data)))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        if self.test_targets:\n",
    "            test_img_path = data['t_path']\n",
    "            test_img = Image.open(test_img_path).convert(\"RGB\")\n",
    "            if self.transform is not None:\n",
    "                test_img = self.transform(test_img)\n",
    "            return {'t_img': test_img,\n",
    "                    't_id': data['t_id']}\n",
    "        else:\n",
    "            c_img_path = data['c_path']\n",
    "            t_img_path = data['t_path']\n",
    "            encoded_caption = data['encoded_caption']\n",
    "\n",
    "            c_img = Image.open(c_img_path).convert(\"RGB\")\n",
    "            if self.transform is not None:\n",
    "                c_img = self.transform(c_img)\n",
    "\n",
    "            t_img = Image.open(t_img_path).convert(\"RGB\")\n",
    "            if self.transform is not None:\n",
    "                t_img = self.transform(t_img)\n",
    "\n",
    "            out = {'c_img': c_img,\n",
    "                   't_img': t_img,\n",
    "                   'encoded_caption': encoded_caption}\n",
    "\n",
    "            if self.split is not 'train':\n",
    "                out['c_id'] = data['c_id']\n",
    "                out['t_id'] = data['t_id']\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_loader(self, batch_size):\n",
    "        return torch.utils.data.DataLoader(self, batch_size=batch_size, \n",
    "                                           shuffle=True if self.split is 'train' else False, \n",
    "                                           num_workers=0, pin_memory=True,\n",
    "                                           drop_last=True if self.split is 'train' else False,\n",
    "                                           collate_fn= None if self.test_targets else self.collate_fn)  #num_workers=8\n",
    "    \n",
    "    def get_img(self, idx, raw_img=False):\n",
    "        img_path = self.data[idx]['c_path']\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if raw_img:\n",
    "          return img\n",
    "        if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "    \n",
    "        batch.sort(key=lambda x: len(x['encoded_caption']), reverse=True)\n",
    "        elem = batch[0]\n",
    "        elem_type = type(elem)\n",
    "\n",
    "        return_batch = {}\n",
    "        for key in elem:     \n",
    "            if key is 'encoded_caption':\n",
    "                return_batch[key] = self.tokenizer.pad([d[key] for d in batch], return_tensors=\"pt\")\n",
    "            else:\n",
    "                return_batch[key] = torch.utils.data._utils.collate.default_collate([d[key] for d in batch])\n",
    "\n",
    "        return return_batch\n",
    "    \n",
    "    def indices_to_string(self, input_ids, skip_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Convert word indices (torch.Tensor) to sentence (string).\n",
    "        \"\"\"\n",
    "        \n",
    "        text = self.tokenizer.decode(input_ids, skip_special_tokens=skip_special_tokens)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">torchvision.transforms\n",
    "\n",
    "Transforms are common image transformations. They can be chained together using *Compose*. Additionally, there is the *torchvision.transforms.functional* module. Functional transforms give fine-grained control over the transformations. This is useful if you have to build a more complex transformation pipeline.\n",
    "\n",
    ">torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "Horizontally flip the given image randomly with a given probability. \n",
    ">torchvision.transforms.RandomAffine\n",
    "\n",
    "Random affine transformation of the image keeping center invariant. \n",
    "\n",
    "* degrees – Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). \n",
    "* translate – tuple of maximum absolute fraction for horizontal and vertical translations. For example translate=(a, b), then horizontal shift is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n",
    "* scale – scaling factor interval, e.g (a, b), then scale is randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n",
    ">torchvision.transforms.ToTensor\n",
    "\n",
    "Convert a PIL Image or numpy.ndarray to tensor. This transform does not support torchscript.\n",
    "\n",
    "Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8. In the other cases, tensors are returned without scaling. \n",
    ">torchvision.transforms.Normalize(mean, std, inplace=False)\n",
    "\n",
    "Normalize a tensor image with mean and standard deviation. This transform does not support PIL Image. Given mean: (mean[1],...,mean[n]) and std: (std[1],..,std[n]) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e., output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\n",
    "* mean – Sequence of means for each channel.\n",
    "* std – Sequence of standard deviations for each channel.\n",
    "\n",
    "### Use *Compose* to perform data pre-processing (data normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/data/fashion_iq/data'\n",
    "val_data_names = ['dress', 'shirt', 'toptee']\n",
    "batch_size = 32\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = T.Compose([\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomAffine(degrees=45, translate=(0.15, 0.15), scale=(0.9, 1.1)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=norm_mean, std=norm_std),\n",
    "    ])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=norm_mean, std=norm_std),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='loading dress train data', max=5985.0, style=ProgressStyl…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "201634dd87354e66ba47b28143cd6c0e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='loading shirt train data', max=5988.0, style=ProgressStyl…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0172d4f6d0b64e21a797a298c6dd5460"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='loading toptee train data', max=6027.0, style=ProgressSty…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a9a4d7960f74c4494201f6e9850cc81"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nStatistics: collected in ['dress', 'shirt', 'toptee'] train, 18000 datas found\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='generating dress val data', max=2017.0, style=ProgressSty…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d600fcf4383748bfa37d14699da27ee0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nStatistics: collected in dress val, 2017 datas found\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='loading dress val pool data', max=3817.0, style=ProgressS…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0580b3a46954a689ac2b8378e747828"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nStatistics: collected in dress val, 3817 datas found\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='generating shirt val data', max=2038.0, style=ProgressSty…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "344cea3e42324370893c8ca5c4af68ac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nStatistics: collected in shirt val, 2038 datas found\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='loading shirt val pool data', max=6346.0, style=ProgressS…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d52a98b68dff4990a8cd3b030be737d7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nStatistics: collected in shirt val, 6346 datas found\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='generating toptee val data', max=1961.0, style=ProgressSt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26f7991ac9fa4544ac6ab9b217768d18"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nStatistics: collected in toptee val, 1961 datas found\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='loading toptee val pool data', max=5373.0, style=Progress…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1ddea6edde1440784845f674f2ce729"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nStatistics: collected in toptee val, 5373 datas found\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FashionIQDataset(data_path, 'train', transform=train_transform)\n",
    "#using split as a flag to train or test\n",
    "#the data_name should be specified as one of 'dress', 'shirt' and 'toptee'\n",
    "train_loader = train_dataset.get_loader(batch_size=batch_size)\n",
    "\n",
    "val_datasets = {}\n",
    "val_pool_datasets = {}\n",
    "for data_name in val_data_names:\n",
    "    val_datasets[data_name] =  FashionIQDataset(data_path, 'val', data_name=data_name, transform=val_transform)\n",
    "    val_pool_datasets[data_name] = FashionIQDataset(data_path, 'val', data_name=data_name, transform=val_transform, test_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "val_dataloaders = []\n",
    "for data_name in val_data_names:\n",
    "    val_dataloaders.append(val_datasets[data_name].get_loader(batch_size))\n",
    "    val_dataloaders.append(val_pool_datasets[data_name].get_loader(batch_size))\n",
    "print(len(val_dataloaders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n",
    "### 2.Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--img_model', default='resnet34', choices=['resnet18', 'resnet34','resnet50'])\n",
    "parser.add_argument('--text_model', default='lstm', choices=['lstm', 'gru', 'transformer', 'bert-base-uncased'])\n",
    "parser.add_argument('--embed_dim', type=int, default=512)\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--head_num', type=int, default=4)\n",
    "parser.add_argument('--layers_num', type=int, default=6)\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=80)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.vocab_size = train_dataset.tokenizer.vocab_size\n",
    "if args.text_model == 'bert-base-cased':\n",
    "    args.hidden_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relvant modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">torch.nn.Module\n",
    "\n",
    "Base class for all neural network modules.Your models should also subclass this class.Modules can also contain other Modules, allowing to nest them in a tree structure. \n",
    "\n",
    "### Creat some classes and optional text models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConCatModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConCatModule, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class NormalizationLayer(torch.nn.Module):\n",
    "    \"\"\"Class for normalization layer.\"\"\"\n",
    "    def __init__(self, normalize_scale=1.0, learn_scale=True):\n",
    "        super(NormalizationLayer, self).__init__()\n",
    "        self.norm_s = float(normalize_scale)\n",
    "        if learn_scale:\n",
    "            self.norm_s = torch.nn.Parameter(torch.FloatTensor((self.norm_s,)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.norm_s * x / torch.norm(x, dim=1, keepdim=True).expand_as(x)\n",
    "        return features\n",
    "    \n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(args.hidden_dim, args.embed_dim),\n",
    "                                 nn.Dropout(p=0.1),)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        return self.net(first_token_tensor)\n",
    "    \n",
    "class TextModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "\n",
    "        super().__init__()\n",
    "        self.text_model = args.text_model\n",
    "        if args.text_model == 'lstm':\n",
    "            self.embedding_layer = nn.Embedding(args.vocab_size, args.embed_dim)\n",
    "            self.net = nn.LSTM(args.embed_dim, args.hidden_dim)\n",
    "        elif args.text_model == 'gru':\n",
    "            self.embedding_layer = nn.Embedding(args.vocab_size, args.embed_dim)\n",
    "            self.net = nn.GRU(args.embed_dim, args.hidden_dim)\n",
    "        elif args.text_model == 'transformer':\n",
    "            self.embedding_layer = nn.Embedding(args.vocab_size, args.embed_dim)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=args.embed_dim, \n",
    "                                                       nhead=args.head_num, \n",
    "                                                       dim_feedforward=args.hidden_dim)\n",
    "            self.net = nn.TransformerEncoder(encoder_layer, num_layers=args.layers_num)\n",
    "        elif 'bert' in args.text_model:\n",
    "            self.net = AutoModelForSequenceClassification.from_pretrained(args.text_model,\n",
    "                                                                          num_labels=args.embed_dim)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.fc_output = nn.Sequential(nn.Dropout(p=0.1),\n",
    "                                       nn.Linear(args.hidden_dim, args.embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.text_model is 'lstm':\n",
    "            x = self.embedding_layer(x)\n",
    "            output, (h_n, c_n) = self.net(x)\n",
    "            text_features = self.fc_output(h_n.squeeze(dim=0))\n",
    "        elif self.text_model is 'transformer':\n",
    "            x = self.embedding_layer(x)\n",
    "            output = self.net(x)\n",
    "            text_features = self.fc_output(output[:, -1, :])\n",
    "        elif 'bert' in args.text_model:\n",
    "            output = self.net(x)\n",
    "            text_features = output.logits\n",
    "#             text_features = self.fc_output(output.last_hidden_state[:, 0])\n",
    "        return text_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PyTorch Lightning \n",
    "\n",
    "Lightning is just organized PyTorch.It forces the following structure to your code which makes it reusable and shareable:\n",
    "1. Research code (the LightningModule).\n",
    "2. Engineering code (you delete, and is handled by the Trainer).\n",
    "3. Non-essential research code (logging, etc... this goes in Callbacks).\n",
    "4. Data (use PyTorch Dataloaders or organize them into a LightningDataModule).\n",
    "\n",
    "### First, encode the query (or reference) image x using a ResNet-17 CNN to get a 2d spatial feature vector φx. Next encode the query text t using a standard LSTM. Define φt to be the hidden state at the final time step whose size d is 512. Keep the text encoder as simple as possible. Finally, combine the two features to compute φxt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIRG(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(args)\n",
    "        self.normalization_layer = NormalizationLayer(\n",
    "            normalize_scale=4.0, learn_scale=True)\n",
    "        # img model\n",
    "        img_model = eval(f'torchvision.models.{args.img_model}(pretrained=True)')\n",
    "\n",
    "        img_model.fc = torch.nn.Sequential(torch.nn.Linear(img_model.fc.in_features, args.embed_dim))\n",
    "        self.img_model = img_model\n",
    "\n",
    "        # text model\n",
    "        self.text_model = TextModel(args)  \n",
    "        \n",
    "        self.a = nn.Parameter(torch.tensor([1.0, 10.0, 1.0, 1.0]))\n",
    "        self.gated_feature_composer = nn.Sequential(\n",
    "            ConCatModule(), nn.BatchNorm1d(2 * args.embed_dim), nn.ReLU(),\n",
    "            nn.Linear(2 * args.embed_dim, args.embed_dim))\n",
    "        self.res_info_composer = nn.Sequential(\n",
    "            ConCatModule(), nn.BatchNorm1d(2 * args.embed_dim), nn.ReLU(),\n",
    "            nn.Linear(2 * args.embed_dim, 2 * args.embed_dim), nn.ReLU(),\n",
    "            nn.Linear(2 * args.embed_dim, args.embed_dim))\n",
    "    \n",
    "    def compose_img_text(self, imgs, texts):\n",
    "        img_features = self.img_model(imgs)\n",
    "        if self.hparams.text_model == 'lstm' and texts.shape[0] == imgs.shape[0]:\n",
    "            texts.transpose_(1, 0) # seq, batch\n",
    "        text_features = self.text_model(texts)\n",
    "        \n",
    "        assert img_features.shape == text_features.shape, \\\n",
    "        'img feat {}, text feat {}'.format(img_features.shape, text_features.shape)\n",
    "        \n",
    "        f1 = self.gated_feature_composer((img_features, text_features))\n",
    "        f2 = self.res_info_composer((img_features, text_features))\n",
    "        f = torch.sigmoid(f1) * img_features * self.a[0] + f2 * self.a[1]\n",
    "        return f\n",
    "    \n",
    "    def compute_batch_based_classification_loss_(self, mod_img1, img2):\n",
    "        x = torch.mm(mod_img1, img2.transpose(0, 1))\n",
    "        labels = torch.tensor(range(x.shape[0])).long().to(self.device)\n",
    "        return F.cross_entropy(x, labels)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "    \n",
    "        c_img, t_img, encoded_caption = batch['c_img'], batch['t_img'], batch['encoded_caption']\n",
    "        mod_img1 = self.compose_img_text(c_img, encoded_caption[\"input_ids\"])\n",
    "        mod_img1 = self.normalization_layer(mod_img1)\n",
    "        \n",
    "        img2 = self.img_model(t_img)\n",
    "        img2 = self.normalization_layer(img2)\n",
    "        \n",
    "        assert (mod_img1.shape[0] == img2.shape[0] and\n",
    "                mod_img1.shape[1] == img2.shape[1])\n",
    "        \n",
    "        loss = self.compute_batch_based_classification_loss_(mod_img1, img2)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx):\n",
    "        if dataloader_idx & 1 == 0: # queries\n",
    "            c_img, encoded_caption = batch['c_img'], batch['encoded_caption']\n",
    "            f = self.compose_img_text(c_img, encoded_caption[\"input_ids\"])\n",
    "            return {'features': f.cpu(), 'c_ids': batch['c_id'].cpu(), 't_ids': batch['t_id'].cpu()}\n",
    "        else: # targets\n",
    "            t_img = batch['t_img']\n",
    "            f = self.img_model(t_img)       \n",
    "            return f.cpu()\n",
    "        \n",
    "        \n",
    "    def validation_epoch_end(self, step_outputs):\n",
    "        for idx in range(0, len(step_outputs), 2):\n",
    "            queries, all_imgs = step_outputs[idx], step_outputs[idx+1]\n",
    "            all_queries = [item['features'] for item in queries]\n",
    "            all_queries = torch.cat(all_queries)\n",
    "            all_imgs = torch.cat(all_imgs)\n",
    "\n",
    "            all_queries_id = [item['c_ids'] for item in queries]\n",
    "            all_queries_id = torch.cat(all_queries_id)\n",
    "            all_targets_id = [item['t_ids'] for item in queries]\n",
    "            all_targets_id = torch.cat(all_targets_id)\n",
    "\n",
    "\n",
    "            # feature normalization\n",
    "            all_queries /= torch.norm(all_queries, dim=1)[:, None]\n",
    "            all_imgs /= torch.norm(all_imgs, dim=1)[:, None]\n",
    "\n",
    "            torch.save(all_imgs, \"all_imgs.pt\")\n",
    "        \n",
    "            # match test queries to target images, get nearest neighbors\n",
    "            sims = all_queries.mm(all_imgs.T)\n",
    "            for i, t in enumerate(all_queries_id):\n",
    "                sims[i, t] = -10e10  # remove query image\n",
    "            nn_result = [torch.argsort(-sims[i, :])[:50] for i in range(sims.shape[0])]\n",
    "\n",
    "            # compute recalls\n",
    "            for k in [1, 5, 10, 50]:\n",
    "                r = 0.0\n",
    "                for i, nns in enumerate(nn_result):\n",
    "                    if all_targets_id[i] in nns[:k]:\n",
    "                        r += 1\n",
    "                r /= len(nn_result)\n",
    "                self.log('{} val_recall_top {}'.format(idx, k), r)\n",
    "\n",
    "            \n",
    "    def test_step(self, batch, batch_idx, dataloader_idx):\n",
    "        if dataloader_idx & 1 == 0: # queries\n",
    "            c_img, encoded_caption = batch['c_img'], batch['encoded_caption']\n",
    "            f = self.compose_img_text(c_img, encoded_caption[\"input_ids\"])\n",
    "            return {'features': f.cpu(), 'c_ids': batch['c_id'].cpu(), 't_ids': batch['t_id'].cpu()}\n",
    "        else: # targets\n",
    "            t_img = batch['t_img']\n",
    "            f = self.img_model(t_img)       \n",
    "            return f.cpu()\n",
    "        \n",
    "        \n",
    "    def test_epoch_end(self, step_outputs):\n",
    "        for idx in range(0, len(step_outputs), 2):\n",
    "            queries, all_imgs = step_outputs[idx], step_outputs[idx+1]\n",
    "            all_queries = [item['features'] for item in queries]\n",
    "            all_queries = torch.cat(all_queries)\n",
    "            all_imgs = torch.cat(all_imgs)\n",
    "\n",
    "            all_queries_id = [item['c_ids'] for item in queries]\n",
    "            all_queries_id = torch.cat(all_queries_id)\n",
    "            all_targets_id = [item['t_ids'] for item in queries]\n",
    "            all_targets_id = torch.cat(all_targets_id)\n",
    "\n",
    "\n",
    "            # feature normalization\n",
    "            all_queries /= torch.norm(all_queries, dim=1)[:, None]\n",
    "            all_imgs /= torch.norm(all_imgs, dim=1)[:, None]\n",
    "\n",
    "            # match test queries to target images, get nearest neighbors\n",
    "            sims = all_queries.mm(all_imgs.T)\n",
    "            for i, t in enumerate(all_queries_id):\n",
    "                sims[i, t] = -10e10  # remove query image\n",
    "            nn_result = [torch.argsort(-sims[i, :])[:50] for i in range(sims.shape[0])]\n",
    "\n",
    "            # compute recalls\n",
    "            for k in [1, 5, 10, 50]:\n",
    "                r = 0.0\n",
    "                for i, nns in enumerate(nn_result):\n",
    "                    if all_targets_id[i] in nns[:k]:\n",
    "                        r += 1\n",
    "                r /= len(nn_result)\n",
    "                self.log('{} test_recall_top {}'.format(idx, k), r)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-6)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type               | Params\n",
      "--------------------------------------------------------------\n",
      "0 | normalization_layer    | NormalizationLayer | 1     \n",
      "1 | img_model              | ResNet             | 21.5 M\n",
      "2 | text_model             | TextModel          | 17.2 M\n",
      "3 | gated_feature_composer | Sequential         | 526 K \n",
      "4 | res_info_composer      | Sequential         | 1.6 M \n",
      "--------------------------------------------------------------\n",
      "40.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "40.9 M    Total params\n",
      "163.442   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f2a7844c2148e88ba757f04788bb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tirg\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# init model\n",
    "model = TIRG(args)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=args.epochs, fast_dev_run=False, num_sanity_val_steps=0, callbacks=[lr_monitor], check_val_every_n_epoch=3)\n",
    "\n",
    "# Train the model ⚡\n",
    "trainer.fit(model, train_loader, val_dataloaders=val_dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Model Evaluating\n",
    "\n",
    "### The metric for retrieval is **recall at rank k (R@K)**, computed as the percentage of test queries where (at least 1) target or correct labeled image is within the top K retrieved images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809e301105fa4510bd1d0157c720def2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'0 test_recall_top 1': 0.030738720872583045,\n",
      " '0 test_recall_top 10': 0.14724838869608328,\n",
      " '0 test_recall_top 5': 0.0961824491819534,\n",
      " '0 test_recall_top 50': 0.3653941497273178,\n",
      " '2 test_recall_top 1': 0.022571148184494603,\n",
      " '2 test_recall_top 10': 0.1280667320902846,\n",
      " '2 test_recall_top 5': 0.0775269872423945,\n",
      " '2 test_recall_top 50': 0.3267909715407262,\n",
      " '4 test_recall_top 1': 0.029576746557878633,\n",
      " '4 test_recall_top 10': 0.1560428352881183,\n",
      " '4 test_recall_top 5': 0.1009688934217236,\n",
      " '4 test_recall_top 50': 0.36970933197348294}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:1 TEST RESULTS\n",
      "{'0 test_recall_top 1': 0.030738720872583045,\n",
      " '0 test_recall_top 10': 0.14724838869608328,\n",
      " '0 test_recall_top 5': 0.0961824491819534,\n",
      " '0 test_recall_top 50': 0.3653941497273178,\n",
      " '2 test_recall_top 1': 0.022571148184494603,\n",
      " '2 test_recall_top 10': 0.1280667320902846,\n",
      " '2 test_recall_top 5': 0.0775269872423945,\n",
      " '2 test_recall_top 50': 0.3267909715407262,\n",
      " '4 test_recall_top 1': 0.029576746557878633,\n",
      " '4 test_recall_top 10': 0.1560428352881183,\n",
      " '4 test_recall_top 5': 0.1009688934217236,\n",
      " '4 test_recall_top 50': 0.36970933197348294}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:2 TEST RESULTS\n",
      "{'0 test_recall_top 1': 0.030738720872583045,\n",
      " '0 test_recall_top 10': 0.14724838869608328,\n",
      " '0 test_recall_top 5': 0.0961824491819534,\n",
      " '0 test_recall_top 50': 0.3653941497273178,\n",
      " '2 test_recall_top 1': 0.022571148184494603,\n",
      " '2 test_recall_top 10': 0.1280667320902846,\n",
      " '2 test_recall_top 5': 0.0775269872423945,\n",
      " '2 test_recall_top 50': 0.3267909715407262,\n",
      " '4 test_recall_top 1': 0.029576746557878633,\n",
      " '4 test_recall_top 10': 0.1560428352881183,\n",
      " '4 test_recall_top 5': 0.1009688934217236,\n",
      " '4 test_recall_top 50': 0.36970933197348294}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:3 TEST RESULTS\n",
      "{'0 test_recall_top 1': 0.030738720872583045,\n",
      " '0 test_recall_top 10': 0.14724838869608328,\n",
      " '0 test_recall_top 5': 0.0961824491819534,\n",
      " '0 test_recall_top 50': 0.3653941497273178,\n",
      " '2 test_recall_top 1': 0.022571148184494603,\n",
      " '2 test_recall_top 10': 0.1280667320902846,\n",
      " '2 test_recall_top 5': 0.0775269872423945,\n",
      " '2 test_recall_top 50': 0.3267909715407262,\n",
      " '4 test_recall_top 1': 0.029576746557878633,\n",
      " '4 test_recall_top 10': 0.1560428352881183,\n",
      " '4 test_recall_top 5': 0.1009688934217236,\n",
      " '4 test_recall_top 50': 0.36970933197348294}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:4 TEST RESULTS\n",
      "{'0 test_recall_top 1': 0.030738720872583045,\n",
      " '0 test_recall_top 10': 0.14724838869608328,\n",
      " '0 test_recall_top 5': 0.0961824491819534,\n",
      " '0 test_recall_top 50': 0.3653941497273178,\n",
      " '2 test_recall_top 1': 0.022571148184494603,\n",
      " '2 test_recall_top 10': 0.1280667320902846,\n",
      " '2 test_recall_top 5': 0.0775269872423945,\n",
      " '2 test_recall_top 50': 0.3267909715407262,\n",
      " '4 test_recall_top 1': 0.029576746557878633,\n",
      " '4 test_recall_top 10': 0.1560428352881183,\n",
      " '4 test_recall_top 5': 0.1009688934217236,\n",
      " '4 test_recall_top 50': 0.36970933197348294}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:5 TEST RESULTS\n",
      "{'0 test_recall_top 1': 0.030738720872583045,\n",
      " '0 test_recall_top 10': 0.14724838869608328,\n",
      " '0 test_recall_top 5': 0.0961824491819534,\n",
      " '0 test_recall_top 50': 0.3653941497273178,\n",
      " '2 test_recall_top 1': 0.022571148184494603,\n",
      " '2 test_recall_top 10': 0.1280667320902846,\n",
      " '2 test_recall_top 5': 0.0775269872423945,\n",
      " '2 test_recall_top 50': 0.3267909715407262,\n",
      " '4 test_recall_top 1': 0.029576746557878633,\n",
      " '4 test_recall_top 10': 0.1560428352881183,\n",
      " '4 test_recall_top 5': 0.1009688934217236,\n",
      " '4 test_recall_top 50': 0.36970933197348294}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'0 test_recall_top 1': 0.030738720872583045,\n",
       "  '0 test_recall_top 5': 0.0961824491819534,\n",
       "  '0 test_recall_top 10': 0.14724838869608328,\n",
       "  '0 test_recall_top 50': 0.3653941497273178,\n",
       "  '2 test_recall_top 1': 0.022571148184494603,\n",
       "  '2 test_recall_top 5': 0.0775269872423945,\n",
       "  '2 test_recall_top 10': 0.1280667320902846,\n",
       "  '2 test_recall_top 50': 0.3267909715407262,\n",
       "  '4 test_recall_top 1': 0.029576746557878633,\n",
       "  '4 test_recall_top 5': 0.1009688934217236,\n",
       "  '4 test_recall_top 10': 0.1560428352881183,\n",
       "  '4 test_recall_top 50': 0.36970933197348294},\n",
       " {'0 test_recall_top 1': 0.030738720872583045,\n",
       "  '0 test_recall_top 5': 0.0961824491819534,\n",
       "  '0 test_recall_top 10': 0.14724838869608328,\n",
       "  '0 test_recall_top 50': 0.3653941497273178,\n",
       "  '2 test_recall_top 1': 0.022571148184494603,\n",
       "  '2 test_recall_top 5': 0.0775269872423945,\n",
       "  '2 test_recall_top 10': 0.1280667320902846,\n",
       "  '2 test_recall_top 50': 0.3267909715407262,\n",
       "  '4 test_recall_top 1': 0.029576746557878633,\n",
       "  '4 test_recall_top 5': 0.1009688934217236,\n",
       "  '4 test_recall_top 10': 0.1560428352881183,\n",
       "  '4 test_recall_top 50': 0.36970933197348294},\n",
       " {'0 test_recall_top 1': 0.030738720872583045,\n",
       "  '0 test_recall_top 5': 0.0961824491819534,\n",
       "  '0 test_recall_top 10': 0.14724838869608328,\n",
       "  '0 test_recall_top 50': 0.3653941497273178,\n",
       "  '2 test_recall_top 1': 0.022571148184494603,\n",
       "  '2 test_recall_top 5': 0.0775269872423945,\n",
       "  '2 test_recall_top 10': 0.1280667320902846,\n",
       "  '2 test_recall_top 50': 0.3267909715407262,\n",
       "  '4 test_recall_top 1': 0.029576746557878633,\n",
       "  '4 test_recall_top 5': 0.1009688934217236,\n",
       "  '4 test_recall_top 10': 0.1560428352881183,\n",
       "  '4 test_recall_top 50': 0.36970933197348294},\n",
       " {'0 test_recall_top 1': 0.030738720872583045,\n",
       "  '0 test_recall_top 5': 0.0961824491819534,\n",
       "  '0 test_recall_top 10': 0.14724838869608328,\n",
       "  '0 test_recall_top 50': 0.3653941497273178,\n",
       "  '2 test_recall_top 1': 0.022571148184494603,\n",
       "  '2 test_recall_top 5': 0.0775269872423945,\n",
       "  '2 test_recall_top 10': 0.1280667320902846,\n",
       "  '2 test_recall_top 50': 0.3267909715407262,\n",
       "  '4 test_recall_top 1': 0.029576746557878633,\n",
       "  '4 test_recall_top 5': 0.1009688934217236,\n",
       "  '4 test_recall_top 10': 0.1560428352881183,\n",
       "  '4 test_recall_top 50': 0.36970933197348294},\n",
       " {'0 test_recall_top 1': 0.030738720872583045,\n",
       "  '0 test_recall_top 5': 0.0961824491819534,\n",
       "  '0 test_recall_top 10': 0.14724838869608328,\n",
       "  '0 test_recall_top 50': 0.3653941497273178,\n",
       "  '2 test_recall_top 1': 0.022571148184494603,\n",
       "  '2 test_recall_top 5': 0.0775269872423945,\n",
       "  '2 test_recall_top 10': 0.1280667320902846,\n",
       "  '2 test_recall_top 50': 0.3267909715407262,\n",
       "  '4 test_recall_top 1': 0.029576746557878633,\n",
       "  '4 test_recall_top 5': 0.1009688934217236,\n",
       "  '4 test_recall_top 10': 0.1560428352881183,\n",
       "  '4 test_recall_top 50': 0.36970933197348294},\n",
       " {'0 test_recall_top 1': 0.030738720872583045,\n",
       "  '0 test_recall_top 5': 0.0961824491819534,\n",
       "  '0 test_recall_top 10': 0.14724838869608328,\n",
       "  '0 test_recall_top 50': 0.3653941497273178,\n",
       "  '2 test_recall_top 1': 0.022571148184494603,\n",
       "  '2 test_recall_top 5': 0.0775269872423945,\n",
       "  '2 test_recall_top 10': 0.1280667320902846,\n",
       "  '2 test_recall_top 50': 0.3267909715407262,\n",
       "  '4 test_recall_top 1': 0.029576746557878633,\n",
       "  '4 test_recall_top 5': 0.1009688934217236,\n",
       "  '4 test_recall_top 10': 0.1560428352881183,\n",
       "  '4 test_recall_top 50': 0.36970933197348294}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = TIRG.load_from_checkpoint('lightning_logs/version_2/checkpoints/epoch=79.ckpt', vocab_size=train_dataset.tokenizer.vocab_size, embed_dim=512)\n",
    "# trainer = pl.Trainer(gpus=1)\n",
    "\n",
    "trainer.test(model, test_dataloaders=val_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c410b7efeff4c889e2901bc3f28e7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'0 test_recall_top 1': 0.029576746557878633,\n",
      " '0 test_recall_top 10': 0.1560428352881183,\n",
      " '0 test_recall_top 5': 0.1009688934217236,\n",
      " '0 test_recall_top 50': 0.36970933197348294,\n",
      " '2 test_recall_top 1': 0.022571148184494603,\n",
      " '2 test_recall_top 10': 0.1280667320902846,\n",
      " '2 test_recall_top 5': 0.0775269872423945,\n",
      " '2 test_recall_top 50': 0.3267909715407262,\n",
      " '4 test_recall_top 1': 0.029576746557878633,\n",
      " '4 test_recall_top 10': 0.1560428352881183,\n",
      " '4 test_recall_top 5': 0.1009688934217236,\n",
      " '4 test_recall_top 50': 0.36970933197348294}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:1 TEST RESULTS\n",
      "{'0 test_recall_top 1': 0.029576746557878633,\n",
      " '0 test_recall_top 10': 0.1560428352881183,\n",
      " '0 test_recall_top 5': 0.1009688934217236,\n",
      " '0 test_recall_top 50': 0.36970933197348294,\n",
      " '2 test_recall_top 1': 0.022571148184494603,\n",
      " '2 test_recall_top 10': 0.1280667320902846,\n",
      " '2 test_recall_top 5': 0.0775269872423945,\n",
      " '2 test_recall_top 50': 0.3267909715407262,\n",
      " '4 test_recall_top 1': 0.029576746557878633,\n",
      " '4 test_recall_top 10': 0.1560428352881183,\n",
      " '4 test_recall_top 5': 0.1009688934217236,\n",
      " '4 test_recall_top 50': 0.36970933197348294}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'0 test_recall_top 1': 0.029576746557878633,\n",
       "  '0 test_recall_top 5': 0.1009688934217236,\n",
       "  '0 test_recall_top 10': 0.1560428352881183,\n",
       "  '0 test_recall_top 50': 0.36970933197348294,\n",
       "  '2 test_recall_top 1': 0.022571148184494603,\n",
       "  '2 test_recall_top 5': 0.0775269872423945,\n",
       "  '2 test_recall_top 10': 0.1280667320902846,\n",
       "  '2 test_recall_top 50': 0.3267909715407262,\n",
       "  '4 test_recall_top 1': 0.029576746557878633,\n",
       "  '4 test_recall_top 5': 0.1009688934217236,\n",
       "  '4 test_recall_top 10': 0.1560428352881183,\n",
       "  '4 test_recall_top 50': 0.36970933197348294},\n",
       " {'0 test_recall_top 1': 0.029576746557878633,\n",
       "  '0 test_recall_top 5': 0.1009688934217236,\n",
       "  '0 test_recall_top 10': 0.1560428352881183,\n",
       "  '0 test_recall_top 50': 0.36970933197348294,\n",
       "  '2 test_recall_top 1': 0.022571148184494603,\n",
       "  '2 test_recall_top 5': 0.0775269872423945,\n",
       "  '2 test_recall_top 10': 0.1280667320902846,\n",
       "  '2 test_recall_top 50': 0.3267909715407262,\n",
       "  '4 test_recall_top 1': 0.029576746557878633,\n",
       "  '4 test_recall_top 5': 0.1009688934217236,\n",
       "  '4 test_recall_top 10': 0.1560428352881183,\n",
       "  '4 test_recall_top 50': 0.36970933197348294}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_name = 'toptee'\n",
    "# trainer.test(model, test_dataloaders=[val_datasets[data_name].get_loader(batch_size), val_pool_datasets[data_name].get_loader(batch_size)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Image Retrieving  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "link = \"try.jpg\"\n",
    "with open(link,\"rb\") as f:\n",
    "    query_img_raw = PIL.Image.open(f).convert(\"RGB\")\n",
    "query_img = train_transform(query_img_raw)\n",
    "query_img = [query_img]\n",
    "query_img = torch.stack(query_img).float()\n",
    "query_img = torch.autograd.Variable(query_img)\n",
    "# Thay câu miêu tả: \"replace A with B\"\n",
    "query_text_raw = [\"black toptee\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "query_text = tokenizer(query_text_raw)[\"input_ids\"]\n",
    "query_text = torch.Tensor(query_text)\n",
    "\n",
    "\n",
    "query_feature = model.compose_img_text(query_img.cuda(), query_text.long().cuda()).data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_imgs= torch.load( \"all_imgs.pt\")\n",
    "sims = query_feature.mm(all_imgs.T)\n",
    "nn_result = [torch.argsort(-sims[i, :])[:50] for i in range(sims.shape[0])]\n",
    "\n",
    "c = 5\n",
    "r = 4\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "# Show query\n",
    "fig.add_subplot(r, c, 3)\n",
    "plt.imshow(query_img_raw)\n",
    "plt.title(query_text_raw[0])\n",
    "plt.axis(\"off\")\n",
    "# Show output\n",
    "k = 15\n",
    "\n",
    "for i in range(k):\n",
    "    img = train_dataset.get_img(int(nn_result[0][i]), raw_img=True)\n",
    "    fig.add_subplot(r, c, i+6)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b4c2bd71ed770783f11d7b4ebb76974afecaa1c8c88e599ba55bd1feec7d40b5",
   "display_name": "Python 3.7.10 64-bit ('multimodel': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}